3-1 书本的目录和索引

书的目录
  - 用于查看章节
  - 指明章节名称
  - 指明页数

书的索引
  - 用于关键词的查找
  - 指明关键词
  - 指明页数  

书与搜索引擎
  - 目录页对应正排索引
  - 索引页对应倒排索引 
  
搜索引擎
  - 正排索引
    - 文档 ID 到文档内容、单词的关联关系
  - 倒排索引
    - 单词到文档 ID 的关联关系  
================================================
3-2 正排与倒排索引简介

正排索引
  - 文档 ID 到文档内容、单词的关联关系

倒排索引
  - 单词到文档 ID 的关联关系
  
查询包含"搜索引擎"的文档
  - 通过倒排索引获得"搜索引擎"对应的文档ID
  - 通过正排索引查询第一步获取ID的完整内容
  - 返回用户最终结果  
================================================
3-3 倒排索引详解

倒排索引是搜索引擎的核心, 主要包含两部分
  - 单词词典(Term Dictionary)
  - 倒排列表(Posting List)

单词词典(Term Dictionary)是倒排索引的重要组成
  - 记录所有文档的单词, 一般都比较大
  - 记录单词到倒排列表的关联信息  

单词词典的实现一般用 B+ Tree
  - https://www.cs.usfca.edu/~galles/visualization/BPlusTree.html

倒排列表(Posting List)记录了单词对应的文档集合, 由倒排索引项(Posint)组成
 
倒排索引项(Posting)主要包含如下信息:
  - 文档 Id, 用于获取原始信息
  - 单词频率(TF, Term Frequency),记录该单词在该文档中的出现次数,用于后续相关性算分
  - 位置(Position), 记录单词在文档中的分词位置(多个),用于做词语搜索(Phrase Query)
  - 偏移(Offset), 记录单词在文档的开始和借宿位置, 用于做高亮显示

es 存储的是一个 json 格式的文档,其中包含多个字段,每个字段会有自己的倒排索引  
================================================
3-4 分词介绍

分词是指将文本转换成一系列单词(term or token)的过程, 也叫文本分析, 在 es 中里面也称为 Analysis
文本		elasticsearch 是最流行的搜索引擎
分词结果	elasticsearch 流行 搜索引擎

分词器是 es 中专门处理分词的组件, 英文为 Analyzer, 组成如下:
  - Character Filters
    - 针对原始文本进行处理, 比如去除html特殊标记符
  - Tokenizer
    - 将原始文本按照一定规则切分为单词
  - Token Filters
    - 针对 tokenizer 处理的单词就行再加工,比如转小写,删除或新增等处理

分词器 - 调用顺序
例如: The QUICK brown foxes jumped over the lazy dog!
Character Filters->Tokenizer->Token Filters-
得出结果[the,quick,brown,foxes,jumped,over,the,lazy,dog]
================================================
3-5 analyze api

es提供了一个测试分词的api接口,方便验证分词效果,endpoint是_analyze
  - 可以直接指定analyzer进行测试
  - 可以直接指定索引中的字段进行测试
  - 可以自定义分词器进行测试

直接指定analyzer进行测试:
POST _analyze
{
  "analyzer":"standard", //分词器
  "text":"hello world!"  //测试文本
}

直接指定索引中的字段进行测试:
POST test_index/_analyze
{
  "field":"username",    //测试字段
  "text":"hello world!"  //测试文本
}

自定义分词器进行测试:
POST _analyze
{
  "tokenizer":"standard", 
  "filter": ["lowercase"],
  "text":"HeLlo WorlD!"  
}
================================================
3-6 自带分词器

es 自带如下分词器:
  - Standard
  - Simple
  - Whitespace
  - Stop
  - Keyword
  - Pattern
  - Language
  
Standard Analyzer
  - 默认分词器
  - 特性
    - 按词切分, 支持多语言
    - 小写处理
  - 组成
    Tokenizer:   Standard
	TokenFilters:Standard,Lower case,Stop(disabled by default)
  - 测试该分词器
  POST _analyze
  {
    "analyzer":"standard", 
    "text":"The 2 2 QUICK Brown Foxes jumped over the lazy dog's bone."  
  }
  
Simple Analyzer
  - 特性
    - 按照非字母切分(非字母丢弃)
	- 小写处理
  - 组成
    Tokenizer: Lower Case
  - 测试该分词器
  POST _analyze
  {
    "analyzer":"simple", 
    "text":"The 2 2 QUICK Brown Foxes jumped over the lazy dog's bone."  
  }

Whitespace Analyzer
  - 特性
    - 按照空格切分
  - 组成
    Tokenizer: Whitespace
  - 测试该分词器
  POST _analyze
  {
    "analyzer":"whitespace", 
    "text":"The 2 2 QUICK Brown Foxes jumped over the lazy dog's bone."  
  }

Stop Analyzer
  - 特性
    - Stop word指语气助词等修饰的词语,比如the,an,的,这等等
  - 组成
    Tokenizer: Lower Case
	TokenFilters:Stop
  - 测试该分词器
  POST _analyze
  {
    "analyzer":"stop", 
    "text":"The 2 2 QUICK Brown Foxes jumped over the lazy dog's bone."  
  }

Keyword Analyzer
  - 特性
    - 不分词,直接将输入作为一个单词输出
  - 组成
    Tokenizer: keyword
  - 测试该分词器
  POST _analyze
  {
    "analyzer":"keyword", 
    "text":"The 2 2 QUICK Brown Foxes jumped over the lazy dog's bone."  
  }

Pattern Analyzer
  - 特性
    - 通过正则表达式自定义分割符
	- 默认是\W+,即非字词的符号作为分割符
  - 组成
    Tokenizer: Pattern
	TokenFilters:Lower Case,Stop(disabled by default)
  - 测试该分词器
  POST _analyze
  {
    "analyzer":"pattern", 
    "text":"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."  
  }  
  
Language Analyzer
  - 提供了30+常见语言的分词器
  - arabic,armenian,basque,bengali,brazilian,czech,english...  
================================================
3-7 中文分词

难点
  - 中文分词是指将一个汉字序列切分成一个个单独的词.在英文中,单词之间是用空格作为自然分界符,汉语中词没有一个形式上的分界符
  - 上下文不同,分词结果迥异,比如交叉歧义问题,比如:
    - 乒乓球拍/卖/完了
    - 乒乓球/拍卖/完了
  - http://mp.weixin.qq.com/s/SiHSMrn8lxCmrtHbcwL-NQ
  
常用分词系统
  - IK
    - 实现中英文单词的切分,支持ik_smart,ik_maxword等模式
	- 可自定义词库,支持热更新分词词典
	- https://github.com/medcl/elasticsearch-analysis-ik
  - jieba  
    - python中最流行的分词系统,支持分词和词性标注
	- 支持繁体分词,自定义词典,并行分词等
	- https://github.com/sing1ee/elasticsearch-jieba-plugin

基于自然语言处理的分词系统
  - Hanlp
    - 由一系列模型与算法组成的Java工具包,目标是普及自然语言处理在生产环境中的应用
    - https://github.com/hankcs/HanLP

  - THULAC	
    - THU Lexical Analyzer for Chinese,由清华大学自然语言处理与社会人文计算实验室研制推出的一套中文词法分析工具包,具有中文分词和词性标注功能
    - https://github.com/microbun/elasticsearch-thulac-plugin

================================================
3-8 自定义分词之 CharacterFilter

当自带的分词无法满足需求时, 就可以自定义分词
  - 通过自定义 Character Filters, Tokenizer 和 Token Filters 实现

Character Filters  
  - 在 Tokenizer 之前对原始文本进行处理, 比如增加, 删除或替换字符等
  - 自带的如下:
    - HTML Strip 去除 html 标签和转换 html 实体
	- Mapping 进行字符替换操作
	- Pattern Replace 进行正则匹配替换
  - 会影响后续 tokenizer 解析的 position 和 offset 信息
  
Character Filters 测试时的 api
  POST _analyze
  {
    "tokenizer":"keyword",
	"char_filter":["html_strip"],
	"text":"<p>I&apos;m so <b>happy</b>!</p>"
  }  
================================================
3-9 自定义分词之 Tokenizer

Tokenizer
  - 将原始文本按照一定规则切分为单词(term or token)
  - 自带的如下:
    - standard 按照单词进行切分
	- letter 按照非字符类进行切分
	- whitespace 按照空格进行切分
	- UAX URL Email 按照 standard 分割, 但不会分割邮箱和 url
	- NGram 和 Edge NGram 连词分割
	- Path Hierarchy 按照文件路径进行切割

Tokenizer 测试时的 api 
  POST _analyze
  {
    "tokenizer":"path_hierarchy",
	  "text":"/one/two/three"
  } 	 
================================================
3-10 自定义分词之 TokenFilter

Token Filters
  -  对于 tokenizer 输出的单词(term)进行增加、删除、修改等操作
  - 自带的如下:
    - lowercase 将所有 term 转换为小写
	- stop 删除 stop words
	- NGram 和 Edge NGram 连词分割
	- Synonym 添加近义词的 term 
  
Filter 测试时可以采用的 api
  POST _analyze
  {
    "text":"a Hello,world!",
    "tokenizer":"standard",
    "filter":[
      "stop",
      "lowercase",
      {
        "type":"ngram",
        "min_gram":4,
        "max_gram":4
      }
    ]
  }   
================================================
3-11 自定义分词

自定义分词 api
  - 自定义分词需要在索引的配置中设定:
  PUT test_index
  {
    "setting":{
	  "analysis":{
	    "char_filter":{},
		"tokenizer":{},
		"filter":{},
		"analyzer":{}
	  }
	}
  }

示例1
  - Character Filters: HTML Strip
  - Tokenizer: Standard
  - Token Filters: Lower case, ASCII Folding
  PUT test_index
  {
    "settings":{
	  "analysis":{
	    "analyzer": {
		  "my_custom_analyzer":{
		    "type":"custom",
			  "tokenizer":"standard",
			  "char_filter":[
			    "html_strip"
			  ],
			  "filter":[
			    "lowercase",
			    "asciifolding"
			  ]
		  }
		 }
	  }
	 }
  }
  POST test_index/_analyze
  {
    "analyzer":"my_custom_analyzer",
	"text":"Is tHis <b>a box</b>?"
  }

示例2
  - Character Filters: Custom Mapping
  - Tokenizer: Custom Pattern
  - Token Filters: Lower case, Custom Stop
  PUT test_index2
  {
    "settings":{
	  "analysis":{
	    "analyzer": {
		  "my_custom_analyzer":{
		    "type":"custom",
		    "char_filter":[
		      "emoticons"
			],
			"tokenizer":"punctuation",
			"filter":[
			  "lowercase",
	          "english_stop"
			]
		  }
		},
		"tokenizer":{
		  "punctuation":{
		    "type":"pattern",
			"pattern":"[.,!?]"
		  }
		},
		"char_filter":{
		  "emoticons":{
		    "type":"mapping",
			"mappings":[
			  ":) => _happy_",
			  ":( => _sad_"
			]
		  }
		},
		"filter":{
		  "english_stop":{
		    "type":"stop",
			"stopwords":"_english_"
		  }
		}
	  }
	}
  }
  POST test_index2/_analyze
  {
    "analyzer":"my_custom_analyzer",
	"text":"I'm a :) person, and you?"
  }  
================================================
3-12 分词使用说明

分词会在如下两个时机使用:
  - 创建或更新文档时(Index Time), 会对相应的文档进行分词处理
  - 查询时(Search Time), 会对查询语句进行分词
  
索引时分词是通过配置 Index Mapping 中每个字段的 analyzer 属性实现的:
  - 不指定分词时, 使用默认的 standard
  PUT test_index
  {
    "mappings":{
	  "doc":{
	    "title":{
		  "type":"text",
		  "analyzer":"whitespace"
		}
	  }
	}
  }  

查询时分词的指定方式有:
  - 查询的时候通过 analyzer 指定分词器
  - 通过 index mapping 设置 search_analyzer 实现

  POST test_index/_search
  {
    "match":{
	  "message":{
	    "query":"hello",
		"analyzer":"standard"
	  }
	}
  }  
  PUT test_index
  {
    "mappings":{
	  "doc":{
	    "properties":{
		  "title":{
		    "type":"text",
			"analyzer":"whitespace",
			"search_analyzer":"standard"
		  }
		}
	  }
	}
  }
  
一般不需要特别指定查询时分词器, 直接使用索引时分词器即可, 否则会出现无法匹配的情况  

分词的使用建议
  - 明确字段是否需要分词, 不需要分词的字段就将 type 设置为 keyword, 可以节省空间和提高写性能
  - 善用 _analyze API, 查看文档的具体分析结果
  - 动手测试
================================================
3-13 官方文档说明

https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis.html 
 